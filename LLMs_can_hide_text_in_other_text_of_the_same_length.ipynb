{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "531d00d77beb4b059cfe9c517fcac080": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09c8b2732c394a828dca34d1c5ce6320",
              "IPY_MODEL_d890336d01bc457ebd8a6b6c4eac1ab3",
              "IPY_MODEL_564a574dd4684e0e85d16f44826a5e1c"
            ],
            "layout": "IPY_MODEL_d563ca7dcca0498faa680f2301e4eb01"
          }
        },
        "09c8b2732c394a828dca34d1c5ce6320": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e2d601141a0438ca62ba8aaac69029b",
            "placeholder": "​",
            "style": "IPY_MODEL_708c8c5401f84774844598f7f5251cfe",
            "value": "Llama-3-8B-Instruct-Gradient-1048k-Q6_K.(…): 100%"
          }
        },
        "d890336d01bc457ebd8a6b6c4eac1ab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b428ecad72ba4c87aa8c7ce09235477f",
            "max": 6596006496,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d5fb4dab1d14ca6808ed82af6063efb",
            "value": 6596006496
          }
        },
        "564a574dd4684e0e85d16f44826a5e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e269e4bf005a43969598de258caf5e2f",
            "placeholder": "​",
            "style": "IPY_MODEL_49f629cf873f4be086603048a9ce25d7",
            "value": " 6.60G/6.60G [04:38&lt;00:00, 122MB/s]"
          }
        },
        "d563ca7dcca0498faa680f2301e4eb01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e2d601141a0438ca62ba8aaac69029b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "708c8c5401f84774844598f7f5251cfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b428ecad72ba4c87aa8c7ce09235477f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d5fb4dab1d14ca6808ed82af6063efb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e269e4bf005a43969598de258caf5e2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49f629cf873f4be086603048a9ce25d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Calgacus demo\n",
        "\n",
        "In this demo Caesar will showcase the **calgacus** protocol to encode and decode texts as described in the paper [*LLMs can hide text in other text of the same length*](https://arxiv.org/abs/2510.20075).\n",
        "\n",
        "![image](https://www.pipelinecomics.com/wp-content/uploads/2018/04/asterix_v17_caesar_third_person.jpg)\n",
        "\n",
        "In seconds, he will let you to encode your own text into an incospicous stegotext through a secret key of your choice. Anyone knowing the secret key will be able to recover your original text by decoding the stegotext using this same notebook."
      ],
      "metadata": {
        "id": "ORvV5h2EVeST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## On acquiring the libraries\n",
        "\n",
        "In colab, you only need to install the python bindings of `llama.cpp`. For your convenience, he fetched a precompiled wheel, allowing you to be ready in seconds rather than waiting fifteen minutes to compile `llama.cpp` from scratch for colab's GPU.\n",
        "\n",
        "![British queuing](https://www.pipelinecomics.com/wp-content/uploads/2018/03/asterix_v8_men_with_facial_hair.jpeg)\n",
        "\n",
        "If you feel very British, and possess an affection for queues, savoring perhaps a little too much the slow unroll of a solemn parade, you can watch all the sub-packets being compiled from scratch by executing the commented line. This is also the safest option if you wish to run this code on your own machine and ensure it uses the GPU.\n",
        "\n"
      ],
      "metadata": {
        "id": "kkws7Ket6F_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"llama-cpp-python==0.3.16\" --prefer-binary --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121\n",
        "\n",
        "# # Version 0.3.12 matches the one used in the paper on a RTX 4070 mobile.\n",
        "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.3.12 --no-cache-dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpVXz3OSMJLW",
        "outputId": "ccbc56a9-21bc-427d-e400-bd2ccfd8f9a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu121\n",
            "Collecting llama-cpp-python==0.3.16\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.16-cu121/llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl (551.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m551.3/551.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python==0.3.16) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python==0.3.16) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.3.16)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python==0.3.16) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.16) (3.0.3)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from llama_cpp import Llama\n",
        "import numpy as np\n",
        "from huggingface_hub import hf_hub_download"
      ],
      "metadata": {
        "id": "3U7GyulzVn7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some bureaucracy\n",
        "\n",
        "Before you proceed, he needs you to obtain [Permit A38](https://youtu.be/4StpMBjMmlY?si=Y7-gA1T8bligeKIA).\n",
        "\n",
        "<img\n",
        "    src=\"https://www.economymagazine.it/wp-content/uploads/2021/11/1556890022176.jpg\"\n",
        "    alt=\"bureaucracy\"\n",
        "    width=\"450\">\n",
        "\n",
        "That is: some open-source LLMs like Llama are gated, and you must explicitly agree to their license before being allowed to download them.\n",
        "\n",
        "The easiest way is to create a HuggingFace account, go to the [Meta Llama 3 model page](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct), and click *“Agree and access repository.”*\n",
        "\n",
        "Now, to let Colab know that you have agreed, get an access token from HuggingFace with *read* permissions, and add it to Colab (click the key icon on the left, *“Add new secret”*, put `HF_TOKEN` as the name and your token as the value, and be sure to grant this notebook access using the toggle on the left).\n",
        "\n",
        "You’re set!\n",
        "\n",
        "You won’t need to repeat this step as long as you reopen the notebook with your current Google account."
      ],
      "metadata": {
        "id": "3imgxZNYRWw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo\n",
        "\n",
        "He has prepared a collection of LLMs for you to experiment with, the same ones used in the paper.\n",
        "You may, however, edit the code and add any LLM hosted on HuggingFace in GGUF format; it will be downloaded and loaded automatically."
      ],
      "metadata": {
        "id": "m1vtM-jsHoXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select a Model\n",
        "\n",
        "model_options = {\n",
        "    \"Llama-3-8B-Instruct-Gradient-Q6_K\": (\"bartowski/Llama-3-8B-Instruct-Gradient-1048k-GGUF\", \"Llama-3-8B-Instruct-Gradient-1048k-Q6_K.gguf\"),\n",
        "    \"Phi-3-mini-4k-instruct-Q4_K_M\": (\"microsoft/Phi-3-mini-4k-instruct-gguf\", \"Phi-3-mini-4k-instruct-Q4_K_M.gguf\"),\n",
        "    \"Qwen_Qwen3-8B-Q6_K_L.gguf\": (\"bartowski/Qwen_Qwen3-8B-GGUF\",\"Qwen_Qwen3-8B-Q6_K_L.gguf\"),\n",
        "    \"Gemma-2-27B-IT-Q4_0\": (\"google/gemma-2-27b-it-gguf\", \"gemma-2-27b-it-q4_0.gguf\"),\n",
        "    \"Phi-3-medium-4k-instruct-Q4_K_M\": (\"microsoft/Phi-3-medium-4k-instruct-gguf\", \"Phi-3-medium-4k-instruct-Q4_K_M.gguf\"),\n",
        "    \"GPT-2-Q4_K_M\": (\"ggml-org/models\", \"ggml-gpt-2-117M-q4_k_m.gguf\"),\n",
        "}\n",
        "\n",
        "# Dropdown menu for model selection\n",
        "selected_model_name = \"Llama-3-8B-Instruct-Gradient-Q6_K\" #@param [\"Llama-3-8B-Instruct-Gradient-Q6_K\", \"Phi-3-mini-4k-instruct-Q4_K_M\", \"Qwen_Qwen3-8B-Q6_K_L.gguf\", \"Gemma-2-27B-IT-Q4_0\", \"Phi-3-medium-4k-instruct-Q4_K_M\", \"GPT-2-Q4_K_M\"]\n",
        "repo_id, filename = model_options[selected_model_name]\n",
        "\n",
        "print(f\"Selected model: {selected_model_name}\")\n",
        "print(f\"Downloading from repository: {repo_id}\")\n",
        "print(f\"Filename: {filename}\")\n",
        "\n",
        "# Download the model\n",
        "model_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "\n",
        "print(f\"Model downloaded to: {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "531d00d77beb4b059cfe9c517fcac080",
            "09c8b2732c394a828dca34d1c5ce6320",
            "d890336d01bc457ebd8a6b6c4eac1ab3",
            "564a574dd4684e0e85d16f44826a5e1c",
            "d563ca7dcca0498faa680f2301e4eb01",
            "1e2d601141a0438ca62ba8aaac69029b",
            "708c8c5401f84774844598f7f5251cfe",
            "b428ecad72ba4c87aa8c7ce09235477f",
            "9d5fb4dab1d14ca6808ed82af6063efb",
            "e269e4bf005a43969598de258caf5e2f",
            "49f629cf873f4be086603048a9ce25d7"
          ]
        },
        "id": "kSgZ0Z0DVpwQ",
        "outputId": "36f77ce2-2c7c-4d0c-8a5e-28c9f9a27f4a",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected model: Llama-3-8B-Instruct-Gradient-Q6_K\n",
            "Downloading from repository: bartowski/Llama-3-8B-Instruct-Gradient-1048k-GGUF\n",
            "Filename: Llama-3-8B-Instruct-Gradient-1048k-Q6_K.gguf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Llama-3-8B-Instruct-Gradient-1048k-Q6_K.(…):   0%|          | 0.00/6.60G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "531d00d77beb4b059cfe9c517fcac080"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model downloaded to: /root/.cache/huggingface/hub/models--bartowski--Llama-3-8B-Instruct-Gradient-1048k-GGUF/snapshots/de5912bdf2b3e42370249634c5f157307e92ca55/Llama-3-8B-Instruct-Gradient-1048k-Q6_K.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replicability notice\n",
        "\n",
        "To generate or decode the exact stegotexts shown in the paper’s examples, run this notebook on a NVIDIA Ada GPU (e.g. RTX 40XX) using `llama-cpp-python==0.3.12`.\n",
        "\n",
        "Infact, matching the llama-cpp version, random seed, and runtime variables (such as `GGML_CUDA_FORCE_MMQ`) will not be sufficient to obtain here on colab the same LLM behavior of the paper, due to the different low-level implementation of foundational libraries like cuBLAS and FlashAttention in NVIDIA Turing GPUs (like the T4 on colab).\n",
        "\n",
        "This is an instance of a [known replicability problem in deep learning](https://arxiv.org/abs/2408.05148) and a limitation of Calgacus, which requires an identical LLM run between encoder and decoder, producing the exact same logits.\n"
      ],
      "metadata": {
        "id": "xJnzGs9QuIZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "llm = Llama(\n",
        "      model_path=model_path,\n",
        "      n_gpu_layers=-1,  # -1 Offloads all layers to GPU. Put instead a positive number N to offload only the first N, you have to do this for big models and small GPUs\n",
        "      logits_all=True,\n",
        "      n_ctx=4096,\n",
        "      flash_attn=True,\n",
        "      seed=1337,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWBAxDKzV-WD",
        "outputId": "f2078090-16c8-463f-fc20-fb03536a1d51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--bartowski--Llama-3-8B-Instruct-Gradient-1048k-GGUF/snapshots/de5912bdf2b3e42370249634c5f157307e92ca55/Llama-3-8B-Instruct-Gradient-1048k-Q6_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = Llama-3-8B-Instruct-Gradient-1048k\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 1048576\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 2804339712.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 18\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  22:                      quantize.imatrix.file str              = /models/Llama-3-8B-Instruct-Gradient-...\n",
            "llama_model_loader: - kv  23:                   quantize.imatrix.dataset str              = /training_data/groups_merged.txt\n",
            "llama_model_loader: - kv  24:             quantize.imatrix.entries_count i32              = 224\n",
            "llama_model_loader: - kv  25:              quantize.imatrix.chunks_count i32              = 88\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q6_K:  226 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q6_K\n",
            "print_info: file size   = 6.14 GiB (6.56 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 128255 '<|reserved_special_token_250|>' is not marked as EOG\n",
            "load: control token: 128254 '<|reserved_special_token_249|>' is not marked as EOG\n",
            "load: control token: 128253 '<|reserved_special_token_248|>' is not marked as EOG\n",
            "load: control token: 128251 '<|reserved_special_token_246|>' is not marked as EOG\n",
            "load: control token: 128246 '<|reserved_special_token_241|>' is not marked as EOG\n",
            "load: control token: 128243 '<|reserved_special_token_238|>' is not marked as EOG\n",
            "load: control token: 128240 '<|reserved_special_token_235|>' is not marked as EOG\n",
            "load: control token: 128239 '<|reserved_special_token_234|>' is not marked as EOG\n",
            "load: control token: 128238 '<|reserved_special_token_233|>' is not marked as EOG\n",
            "load: control token: 128237 '<|reserved_special_token_232|>' is not marked as EOG\n",
            "load: control token: 128232 '<|reserved_special_token_227|>' is not marked as EOG\n",
            "load: control token: 128228 '<|reserved_special_token_223|>' is not marked as EOG\n",
            "load: control token: 128227 '<|reserved_special_token_222|>' is not marked as EOG\n",
            "load: control token: 128225 '<|reserved_special_token_220|>' is not marked as EOG\n",
            "load: control token: 128222 '<|reserved_special_token_217|>' is not marked as EOG\n",
            "load: control token: 128215 '<|reserved_special_token_210|>' is not marked as EOG\n",
            "load: control token: 128211 '<|reserved_special_token_206|>' is not marked as EOG\n",
            "load: control token: 128210 '<|reserved_special_token_205|>' is not marked as EOG\n",
            "load: control token: 128204 '<|reserved_special_token_199|>' is not marked as EOG\n",
            "load: control token: 128203 '<|reserved_special_token_198|>' is not marked as EOG\n",
            "load: control token: 128201 '<|reserved_special_token_196|>' is not marked as EOG\n",
            "load: control token: 128197 '<|reserved_special_token_192|>' is not marked as EOG\n",
            "load: control token: 128196 '<|reserved_special_token_191|>' is not marked as EOG\n",
            "load: control token: 128195 '<|reserved_special_token_190|>' is not marked as EOG\n",
            "load: control token: 128193 '<|reserved_special_token_188|>' is not marked as EOG\n",
            "load: control token: 128191 '<|reserved_special_token_186|>' is not marked as EOG\n",
            "load: control token: 128190 '<|reserved_special_token_185|>' is not marked as EOG\n",
            "load: control token: 128185 '<|reserved_special_token_180|>' is not marked as EOG\n",
            "load: control token: 128184 '<|reserved_special_token_179|>' is not marked as EOG\n",
            "load: control token: 128182 '<|reserved_special_token_177|>' is not marked as EOG\n",
            "load: control token: 128181 '<|reserved_special_token_176|>' is not marked as EOG\n",
            "load: control token: 128177 '<|reserved_special_token_172|>' is not marked as EOG\n",
            "load: control token: 128176 '<|reserved_special_token_171|>' is not marked as EOG\n",
            "load: control token: 128175 '<|reserved_special_token_170|>' is not marked as EOG\n",
            "load: control token: 128174 '<|reserved_special_token_169|>' is not marked as EOG\n",
            "load: control token: 128173 '<|reserved_special_token_168|>' is not marked as EOG\n",
            "load: control token: 128172 '<|reserved_special_token_167|>' is not marked as EOG\n",
            "load: control token: 128168 '<|reserved_special_token_163|>' is not marked as EOG\n",
            "load: control token: 128167 '<|reserved_special_token_162|>' is not marked as EOG\n",
            "load: control token: 128166 '<|reserved_special_token_161|>' is not marked as EOG\n",
            "load: control token: 128165 '<|reserved_special_token_160|>' is not marked as EOG\n",
            "load: control token: 128162 '<|reserved_special_token_157|>' is not marked as EOG\n",
            "load: control token: 128159 '<|reserved_special_token_154|>' is not marked as EOG\n",
            "load: control token: 128155 '<|reserved_special_token_150|>' is not marked as EOG\n",
            "load: control token: 128153 '<|reserved_special_token_148|>' is not marked as EOG\n",
            "load: control token: 128152 '<|reserved_special_token_147|>' is not marked as EOG\n",
            "load: control token: 128151 '<|reserved_special_token_146|>' is not marked as EOG\n",
            "load: control token: 128148 '<|reserved_special_token_143|>' is not marked as EOG\n",
            "load: control token: 128146 '<|reserved_special_token_141|>' is not marked as EOG\n",
            "load: control token: 128144 '<|reserved_special_token_139|>' is not marked as EOG\n",
            "load: control token: 128143 '<|reserved_special_token_138|>' is not marked as EOG\n",
            "load: control token: 128141 '<|reserved_special_token_136|>' is not marked as EOG\n",
            "load: control token: 128139 '<|reserved_special_token_134|>' is not marked as EOG\n",
            "load: control token: 128138 '<|reserved_special_token_133|>' is not marked as EOG\n",
            "load: control token: 128135 '<|reserved_special_token_130|>' is not marked as EOG\n",
            "load: control token: 128133 '<|reserved_special_token_128|>' is not marked as EOG\n",
            "load: control token: 128132 '<|reserved_special_token_127|>' is not marked as EOG\n",
            "load: control token: 128131 '<|reserved_special_token_126|>' is not marked as EOG\n",
            "load: control token: 128130 '<|reserved_special_token_125|>' is not marked as EOG\n",
            "load: control token: 128128 '<|reserved_special_token_123|>' is not marked as EOG\n",
            "load: control token: 128125 '<|reserved_special_token_120|>' is not marked as EOG\n",
            "load: control token: 128121 '<|reserved_special_token_116|>' is not marked as EOG\n",
            "load: control token: 128120 '<|reserved_special_token_115|>' is not marked as EOG\n",
            "load: control token: 128119 '<|reserved_special_token_114|>' is not marked as EOG\n",
            "load: control token: 128116 '<|reserved_special_token_111|>' is not marked as EOG\n",
            "load: control token: 128112 '<|reserved_special_token_107|>' is not marked as EOG\n",
            "load: control token: 128109 '<|reserved_special_token_104|>' is not marked as EOG\n",
            "load: control token: 128107 '<|reserved_special_token_102|>' is not marked as EOG\n",
            "load: control token: 128106 '<|reserved_special_token_101|>' is not marked as EOG\n",
            "load: control token: 128105 '<|reserved_special_token_100|>' is not marked as EOG\n",
            "load: control token: 128103 '<|reserved_special_token_98|>' is not marked as EOG\n",
            "load: control token: 128100 '<|reserved_special_token_95|>' is not marked as EOG\n",
            "load: control token: 128099 '<|reserved_special_token_94|>' is not marked as EOG\n",
            "load: control token: 128098 '<|reserved_special_token_93|>' is not marked as EOG\n",
            "load: control token: 128094 '<|reserved_special_token_89|>' is not marked as EOG\n",
            "load: control token: 128088 '<|reserved_special_token_83|>' is not marked as EOG\n",
            "load: control token: 128087 '<|reserved_special_token_82|>' is not marked as EOG\n",
            "load: control token: 128086 '<|reserved_special_token_81|>' is not marked as EOG\n",
            "load: control token: 128084 '<|reserved_special_token_79|>' is not marked as EOG\n",
            "load: control token: 128082 '<|reserved_special_token_77|>' is not marked as EOG\n",
            "load: control token: 128078 '<|reserved_special_token_73|>' is not marked as EOG\n",
            "load: control token: 128075 '<|reserved_special_token_70|>' is not marked as EOG\n",
            "load: control token: 128073 '<|reserved_special_token_68|>' is not marked as EOG\n",
            "load: control token: 128072 '<|reserved_special_token_67|>' is not marked as EOG\n",
            "load: control token: 128070 '<|reserved_special_token_65|>' is not marked as EOG\n",
            "load: control token: 128065 '<|reserved_special_token_60|>' is not marked as EOG\n",
            "load: control token: 128064 '<|reserved_special_token_59|>' is not marked as EOG\n",
            "load: control token: 128062 '<|reserved_special_token_57|>' is not marked as EOG\n",
            "load: control token: 128060 '<|reserved_special_token_55|>' is not marked as EOG\n",
            "load: control token: 128059 '<|reserved_special_token_54|>' is not marked as EOG\n",
            "load: control token: 128057 '<|reserved_special_token_52|>' is not marked as EOG\n",
            "load: control token: 128056 '<|reserved_special_token_51|>' is not marked as EOG\n",
            "load: control token: 128054 '<|reserved_special_token_49|>' is not marked as EOG\n",
            "load: control token: 128051 '<|reserved_special_token_46|>' is not marked as EOG\n",
            "load: control token: 128043 '<|reserved_special_token_38|>' is not marked as EOG\n",
            "load: control token: 128042 '<|reserved_special_token_37|>' is not marked as EOG\n",
            "load: control token: 128041 '<|reserved_special_token_36|>' is not marked as EOG\n",
            "load: control token: 128040 '<|reserved_special_token_35|>' is not marked as EOG\n",
            "load: control token: 128035 '<|reserved_special_token_30|>' is not marked as EOG\n",
            "load: control token: 128033 '<|reserved_special_token_28|>' is not marked as EOG\n",
            "load: control token: 128032 '<|reserved_special_token_27|>' is not marked as EOG\n",
            "load: control token: 128029 '<|reserved_special_token_24|>' is not marked as EOG\n",
            "load: control token: 128025 '<|reserved_special_token_20|>' is not marked as EOG\n",
            "load: control token: 128024 '<|reserved_special_token_19|>' is not marked as EOG\n",
            "load: control token: 128021 '<|reserved_special_token_16|>' is not marked as EOG\n",
            "load: control token: 128020 '<|reserved_special_token_15|>' is not marked as EOG\n",
            "load: control token: 128019 '<|reserved_special_token_14|>' is not marked as EOG\n",
            "load: control token: 128018 '<|reserved_special_token_13|>' is not marked as EOG\n",
            "load: control token: 128015 '<|reserved_special_token_10|>' is not marked as EOG\n",
            "load: control token: 128013 '<|reserved_special_token_8|>' is not marked as EOG\n",
            "load: control token: 128012 '<|reserved_special_token_7|>' is not marked as EOG\n",
            "load: control token: 128010 '<|reserved_special_token_5|>' is not marked as EOG\n",
            "load: control token: 128005 '<|reserved_special_token_3|>' is not marked as EOG\n",
            "load: control token: 128004 '<|reserved_special_token_2|>' is not marked as EOG\n",
            "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
            "load: control token: 128249 '<|reserved_special_token_244|>' is not marked as EOG\n",
            "load: control token: 128187 '<|reserved_special_token_182|>' is not marked as EOG\n",
            "load: control token: 128180 '<|reserved_special_token_175|>' is not marked as EOG\n",
            "load: control token: 128134 '<|reserved_special_token_129|>' is not marked as EOG\n",
            "load: control token: 128179 '<|reserved_special_token_174|>' is not marked as EOG\n",
            "load: control token: 128037 '<|reserved_special_token_32|>' is not marked as EOG\n",
            "load: control token: 128045 '<|reserved_special_token_40|>' is not marked as EOG\n",
            "load: control token: 128089 '<|reserved_special_token_84|>' is not marked as EOG\n",
            "load: control token: 128212 '<|reserved_special_token_207|>' is not marked as EOG\n",
            "load: control token: 128104 '<|reserved_special_token_99|>' is not marked as EOG\n",
            "load: control token: 128205 '<|reserved_special_token_200|>' is not marked as EOG\n",
            "load: control token: 128142 '<|reserved_special_token_137|>' is not marked as EOG\n",
            "load: control token: 128028 '<|reserved_special_token_23|>' is not marked as EOG\n",
            "load: control token: 128126 '<|reserved_special_token_121|>' is not marked as EOG\n",
            "load: control token: 128198 '<|reserved_special_token_193|>' is not marked as EOG\n",
            "load: control token: 128071 '<|reserved_special_token_66|>' is not marked as EOG\n",
            "load: control token: 128092 '<|reserved_special_token_87|>' is not marked as EOG\n",
            "load: control token: 128183 '<|reserved_special_token_178|>' is not marked as EOG\n",
            "load: control token: 128140 '<|reserved_special_token_135|>' is not marked as EOG\n",
            "load: control token: 128226 '<|reserved_special_token_221|>' is not marked as EOG\n",
            "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
            "load: control token: 128052 '<|reserved_special_token_47|>' is not marked as EOG\n",
            "load: control token: 128053 '<|reserved_special_token_48|>' is not marked as EOG\n",
            "load: control token: 128058 '<|reserved_special_token_53|>' is not marked as EOG\n",
            "load: control token: 128150 '<|reserved_special_token_145|>' is not marked as EOG\n",
            "load: control token: 128149 '<|reserved_special_token_144|>' is not marked as EOG\n",
            "load: control token: 128209 '<|reserved_special_token_204|>' is not marked as EOG\n",
            "load: control token: 128169 '<|reserved_special_token_164|>' is not marked as EOG\n",
            "load: control token: 128157 '<|reserved_special_token_152|>' is not marked as EOG\n",
            "load: control token: 128038 '<|reserved_special_token_33|>' is not marked as EOG\n",
            "load: control token: 128178 '<|reserved_special_token_173|>' is not marked as EOG\n",
            "load: control token: 128091 '<|reserved_special_token_86|>' is not marked as EOG\n",
            "load: control token: 128115 '<|reserved_special_token_110|>' is not marked as EOG\n",
            "load: control token: 128233 '<|reserved_special_token_228|>' is not marked as EOG\n",
            "load: control token: 128145 '<|reserved_special_token_140|>' is not marked as EOG\n",
            "load: control token: 128039 '<|reserved_special_token_34|>' is not marked as EOG\n",
            "load: control token: 128136 '<|reserved_special_token_131|>' is not marked as EOG\n",
            "load: control token: 128170 '<|reserved_special_token_165|>' is not marked as EOG\n",
            "load: control token: 128236 '<|reserved_special_token_231|>' is not marked as EOG\n",
            "load: control token: 128154 '<|reserved_special_token_149|>' is not marked as EOG\n",
            "load: control token: 128049 '<|reserved_special_token_44|>' is not marked as EOG\n",
            "load: control token: 128023 '<|reserved_special_token_18|>' is not marked as EOG\n",
            "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
            "load: control token: 128016 '<|reserved_special_token_11|>' is not marked as EOG\n",
            "load: control token: 128113 '<|reserved_special_token_108|>' is not marked as EOG\n",
            "load: control token: 128158 '<|reserved_special_token_153|>' is not marked as EOG\n",
            "load: control token: 128223 '<|reserved_special_token_218|>' is not marked as EOG\n",
            "load: control token: 128156 '<|reserved_special_token_151|>' is not marked as EOG\n",
            "load: control token: 128008 '<|reserved_special_token_4|>' is not marked as EOG\n",
            "load: control token: 128085 '<|reserved_special_token_80|>' is not marked as EOG\n",
            "load: control token: 128160 '<|reserved_special_token_155|>' is not marked as EOG\n",
            "load: control token: 128110 '<|reserved_special_token_105|>' is not marked as EOG\n",
            "load: control token: 128247 '<|reserved_special_token_242|>' is not marked as EOG\n",
            "load: control token: 128122 '<|reserved_special_token_117|>' is not marked as EOG\n",
            "load: control token: 128050 '<|reserved_special_token_45|>' is not marked as EOG\n",
            "load: control token: 128221 '<|reserved_special_token_216|>' is not marked as EOG\n",
            "load: control token: 128244 '<|reserved_special_token_239|>' is not marked as EOG\n",
            "load: control token: 128248 '<|reserved_special_token_243|>' is not marked as EOG\n",
            "load: control token: 128213 '<|reserved_special_token_208|>' is not marked as EOG\n",
            "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
            "load: control token: 128208 '<|reserved_special_token_203|>' is not marked as EOG\n",
            "load: control token: 128074 '<|reserved_special_token_69|>' is not marked as EOG\n",
            "load: control token: 128234 '<|reserved_special_token_229|>' is not marked as EOG\n",
            "load: control token: 128083 '<|reserved_special_token_78|>' is not marked as EOG\n",
            "load: control token: 128224 '<|reserved_special_token_219|>' is not marked as EOG\n",
            "load: control token: 128055 '<|reserved_special_token_50|>' is not marked as EOG\n",
            "load: control token: 128097 '<|reserved_special_token_92|>' is not marked as EOG\n",
            "load: control token: 128206 '<|reserved_special_token_201|>' is not marked as EOG\n",
            "load: control token: 128081 '<|reserved_special_token_76|>' is not marked as EOG\n",
            "load: control token: 128068 '<|reserved_special_token_63|>' is not marked as EOG\n",
            "load: control token: 128067 '<|reserved_special_token_62|>' is not marked as EOG\n",
            "load: control token: 128046 '<|reserved_special_token_41|>' is not marked as EOG\n",
            "load: control token: 128194 '<|reserved_special_token_189|>' is not marked as EOG\n",
            "load: control token: 128069 '<|reserved_special_token_64|>' is not marked as EOG\n",
            "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
            "load: control token: 128220 '<|reserved_special_token_215|>' is not marked as EOG\n",
            "load: control token: 128214 '<|reserved_special_token_209|>' is not marked as EOG\n",
            "load: control token: 128108 '<|reserved_special_token_103|>' is not marked as EOG\n",
            "load: control token: 128200 '<|reserved_special_token_195|>' is not marked as EOG\n",
            "load: control token: 128048 '<|reserved_special_token_43|>' is not marked as EOG\n",
            "load: control token: 128027 '<|reserved_special_token_22|>' is not marked as EOG\n",
            "load: control token: 128114 '<|reserved_special_token_109|>' is not marked as EOG\n",
            "load: control token: 128235 '<|reserved_special_token_230|>' is not marked as EOG\n",
            "load: control token: 128252 '<|reserved_special_token_247|>' is not marked as EOG\n",
            "load: control token: 128199 '<|reserved_special_token_194|>' is not marked as EOG\n",
            "load: control token: 128129 '<|reserved_special_token_124|>' is not marked as EOG\n",
            "load: control token: 128245 '<|reserved_special_token_240|>' is not marked as EOG\n",
            "load: control token: 128164 '<|reserved_special_token_159|>' is not marked as EOG\n",
            "load: control token: 128124 '<|reserved_special_token_119|>' is not marked as EOG\n",
            "load: control token: 128102 '<|reserved_special_token_97|>' is not marked as EOG\n",
            "load: control token: 128036 '<|reserved_special_token_31|>' is not marked as EOG\n",
            "load: control token: 128229 '<|reserved_special_token_224|>' is not marked as EOG\n",
            "load: control token: 128163 '<|reserved_special_token_158|>' is not marked as EOG\n",
            "load: control token: 128127 '<|reserved_special_token_122|>' is not marked as EOG\n",
            "load: control token: 128111 '<|reserved_special_token_106|>' is not marked as EOG\n",
            "load: control token: 128231 '<|reserved_special_token_226|>' is not marked as EOG\n",
            "load: control token: 128188 '<|reserved_special_token_183|>' is not marked as EOG\n",
            "load: control token: 128061 '<|reserved_special_token_56|>' is not marked as EOG\n",
            "load: control token: 128137 '<|reserved_special_token_132|>' is not marked as EOG\n",
            "load: control token: 128093 '<|reserved_special_token_88|>' is not marked as EOG\n",
            "load: control token: 128095 '<|reserved_special_token_90|>' is not marked as EOG\n",
            "load: control token: 128189 '<|reserved_special_token_184|>' is not marked as EOG\n",
            "load: control token: 128090 '<|reserved_special_token_85|>' is not marked as EOG\n",
            "load: control token: 128147 '<|reserved_special_token_142|>' is not marked as EOG\n",
            "load: control token: 128219 '<|reserved_special_token_214|>' is not marked as EOG\n",
            "load: control token: 128230 '<|reserved_special_token_225|>' is not marked as EOG\n",
            "load: control token: 128217 '<|reserved_special_token_212|>' is not marked as EOG\n",
            "load: control token: 128031 '<|reserved_special_token_26|>' is not marked as EOG\n",
            "load: control token: 128030 '<|reserved_special_token_25|>' is not marked as EOG\n",
            "load: control token: 128250 '<|reserved_special_token_245|>' is not marked as EOG\n",
            "load: control token: 128192 '<|reserved_special_token_187|>' is not marked as EOG\n",
            "load: control token: 128096 '<|reserved_special_token_91|>' is not marked as EOG\n",
            "load: control token: 128186 '<|reserved_special_token_181|>' is not marked as EOG\n",
            "load: control token: 128207 '<|reserved_special_token_202|>' is not marked as EOG\n",
            "load: control token: 128171 '<|reserved_special_token_166|>' is not marked as EOG\n",
            "load: control token: 128080 '<|reserved_special_token_75|>' is not marked as EOG\n",
            "load: control token: 128077 '<|reserved_special_token_72|>' is not marked as EOG\n",
            "load: control token: 128101 '<|reserved_special_token_96|>' is not marked as EOG\n",
            "load: control token: 128079 '<|reserved_special_token_74|>' is not marked as EOG\n",
            "load: control token: 128216 '<|reserved_special_token_211|>' is not marked as EOG\n",
            "load: control token: 128014 '<|reserved_special_token_9|>' is not marked as EOG\n",
            "load: control token: 128047 '<|reserved_special_token_42|>' is not marked as EOG\n",
            "load: control token: 128202 '<|reserved_special_token_197|>' is not marked as EOG\n",
            "load: control token: 128044 '<|reserved_special_token_39|>' is not marked as EOG\n",
            "load: control token: 128161 '<|reserved_special_token_156|>' is not marked as EOG\n",
            "load: control token: 128017 '<|reserved_special_token_12|>' is not marked as EOG\n",
            "load: control token: 128066 '<|reserved_special_token_61|>' is not marked as EOG\n",
            "load: control token: 128242 '<|reserved_special_token_237|>' is not marked as EOG\n",
            "load: control token: 128118 '<|reserved_special_token_113|>' is not marked as EOG\n",
            "load: control token: 128076 '<|reserved_special_token_71|>' is not marked as EOG\n",
            "load: control token: 128034 '<|reserved_special_token_29|>' is not marked as EOG\n",
            "load: control token: 128241 '<|reserved_special_token_236|>' is not marked as EOG\n",
            "load: control token: 128026 '<|reserved_special_token_21|>' is not marked as EOG\n",
            "load: control token: 128218 '<|reserved_special_token_213|>' is not marked as EOG\n",
            "load: control token: 128063 '<|reserved_special_token_58|>' is not marked as EOG\n",
            "load: control token: 128117 '<|reserved_special_token_112|>' is not marked as EOG\n",
            "load: control token: 128011 '<|reserved_special_token_6|>' is not marked as EOG\n",
            "load: control token: 128022 '<|reserved_special_token_17|>' is not marked as EOG\n",
            "load: control token: 128123 '<|reserved_special_token_118|>' is not marked as EOG\n",
            "load: printing all EOG tokens:\n",
            "load:   - 128001 ('<|end_of_text|>')\n",
            "load:   - 128009 ('<|eot_id|>')\n",
            "load: special tokens cache size = 256\n",
            "load: token to piece cache size = 0.8000 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 1048576\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 14336\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 2804339712.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 1048576\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 8B\n",
            "print_info: model params     = 8.03 B\n",
            "print_info: general.name     = Llama-3-8B-Instruct-Gradient-1048k\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 128256\n",
            "print_info: n_merges         = 280147\n",
            "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
            "print_info: EOS token        = 128001 '<|end_of_text|>'\n",
            "print_info: EOT token        = 128009 '<|eot_id|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
            "print_info: EOG token        = 128009 '<|eot_id|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q6_K) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
            "load_tensors: offloading 32 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 33/33 layers to GPU\n",
            "load_tensors:        CUDA0 model buffer size =  5871.99 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =   410.98 MiB\n",
            ".........................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 1\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 2804339712.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (1048576) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:  CUDA_Host  output buffer size =     0.49 MiB\n",
            "create_memory: n_ctx = 4096 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   1: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   2: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   3: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   4: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   5: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   6: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   7: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   8: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   9: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  10: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  11: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  12: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  13: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  14: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  15: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  16: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  17: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  18: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  19: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  20: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  21: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  22: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  23: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  24: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  25: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  26: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  27: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  28: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  29: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  30: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  31: dev = CUDA0\n",
            "llama_kv_cache_unified:      CUDA0 KV buffer size =   512.00 MiB\n",
            "llama_kv_cache_unified: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 2\n",
            "llama_context: max_nodes = 2328\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:      CUDA0 compute buffer size =   258.50 MiB\n",
            "llama_context:  CUDA_Host compute buffer size =    16.01 MiB\n",
            "llama_context: graph nodes  = 999\n",
            "llama_context: graph splits = 2\n",
            "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'quantize.imatrix.entries_count': '224', 'quantize.imatrix.dataset': '/training_data/groups_merged.txt', 'quantize.imatrix.chunks_count': '88', 'quantize.imatrix.file': '/models/Llama-3-8B-Instruct-Gradient-1048k-GGUF/Llama-3-8B-Instruct-Gradient-1048k.imatrix', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '2804339712.000000', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '1048576', 'general.name': 'Llama-3-8B-Instruct-Gradient-1048k', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '18', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: llama-3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here he exposed the core functions used by calgacus to extract the rank sequence from a text given a llm, and to generate text according to a rank sequence."
      ],
      "metadata": {
        "id": "pa6Kut3-2gTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import time\n",
        "\n",
        "def get_token_ranks_llama_cpp(text: str, model: Llama, prompt: str = 'Some English text:', verbose=False, return_all_logprobs=False) -> List[int]:\n",
        "    \"\"\"\n",
        "    Get the rankings of tokens in a given text using a Llama model.\n",
        "\n",
        "    Args:\n",
        "        text: The text to analyze.\n",
        "        model: The Llama model to use.\n",
        "        prompt: The prompt to prepend to the text.\n",
        "        verbose: Whether to print detailed output.\n",
        "\n",
        "    Returns:\n",
        "        A list of token ranks and cumulative log probability.\n",
        "    \"\"\"\n",
        "    # Tokenize the prompt and the text\n",
        "    prompt_ids = model.tokenize(prompt.encode('utf-8'))[1:]\n",
        "    # Ensure the text is valid UTF-8 before tokenization\n",
        "    text = text.encode('utf-8', errors='ignore').decode('utf-8')  # Ignore or replace invalid characters\n",
        "    text_ids = model.tokenize((' ' + text).encode('utf-8'))[1:]\n",
        "    print(f\"prefix: {prompt} - idseq: {prompt_ids}\")\n",
        "\n",
        "    # Combine prompt and text tokens\n",
        "    input_ids = prompt_ids + text_ids\n",
        "\n",
        "    # Prepare to track ranks\n",
        "    ranks = []\n",
        "    cumulative_logprob = 0\n",
        "    all_logprobs = []\n",
        "\n",
        "    # Reset the model only once and evaluate the prompt\n",
        "    model.reset()\n",
        "    model.eval(prompt_ids)  # Initialize the context with the prompt\n",
        "\n",
        "    # Evaluate the text incrementally\n",
        "    for i in range(len(prompt_ids), len(input_ids)):\n",
        "        # Evaluate only the current token\n",
        "        current_token = input_ids[i]\n",
        "        model.eval([current_token])  # Add the new token to the context\n",
        "\n",
        "        # Get logits for the last predicted token\n",
        "        logits = model.scores[i - 1]\n",
        "\n",
        "        # Convert logits to probabilities\n",
        "        probs = np.exp(logits) / np.sum(np.exp(logits))\n",
        "\n",
        "        # Get the rank of the current token\n",
        "        sorted_indices = np.argsort(logits)[::-1]\n",
        "        rank_np = np.where(sorted_indices == current_token)[0][0] + 1\n",
        "\n",
        "        # Store the rank (1-indexed)\n",
        "        ranks.append(int(rank_np))\n",
        "\n",
        "        try:\n",
        "            token_str = model.detokenize([current_token]).decode('utf-8')  # Try decoding\n",
        "        except UnicodeDecodeError:\n",
        "            token_str = \"[DECODE ERROR]\"  # Handle decode error\n",
        "        token_prob = probs[current_token].item()\n",
        "        # Verbose output\n",
        "        if verbose:\n",
        "            print(f\"{i} Token: '{token_str}' - Rank: {int(rank_np)} - Probability: {token_prob:.6f}\")\n",
        "\n",
        "        # Calculate logprob and add to cumulative\n",
        "        logprob = np.log(token_prob)\n",
        "        cumulative_logprob += logprob\n",
        "        all_logprobs.append(logprob)\n",
        "\n",
        "    print('Cumulative logprob:', cumulative_logprob)\n",
        "    if return_all_logprobs:\n",
        "        return ranks, cumulative_logprob, all_logprobs\n",
        "    return ranks, cumulative_logprob\n",
        "\n",
        "def decode_from_ranks(prompt: str, ranks: List[int], model: Llama, verbose: bool = False, return_all_logprobs: bool = False) -> str:\n",
        "    \"\"\"\n",
        "    Decode text from specified token ranks using a Llama model.\n",
        "\n",
        "    Args:\n",
        "        prompt: The initial prompt text.\n",
        "        ranks: A list of token ranks to decode.\n",
        "        model: The Llama model to use.\n",
        "        verbose: If True, print detailed token information.\n",
        "\n",
        "    Returns:\n",
        "        The decoded text and cumulative log probability.\n",
        "    \"\"\"\n",
        "    print(ranks)\n",
        "    cumulative_logprob = 0\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    input_ids = model.tokenize(prompt.encode('utf-8'))[1:]\n",
        "    if verbose:\n",
        "        print(f\"secret_key: {prompt} - idseq: {input_ids}\")\n",
        "\n",
        "    # Initialize the generated sequence with the prompt tokens\n",
        "    generated = input_ids.copy()\n",
        "\n",
        "    # Reset the model and evaluate the prompt only once\n",
        "    model.reset()\n",
        "    model.eval(input_ids)\n",
        "    all_log_probs = []\n",
        "\n",
        "    for i, rank in enumerate(ranks):\n",
        "        # Get logits for the next token\n",
        "        logits = model.scores[len(generated) - 1]\n",
        "\n",
        "        # Sort logits and get the indices of sorted tokens\n",
        "        sorted_indices = np.argsort(logits)[::-1]\n",
        "\n",
        "        # Get the token corresponding to the desired rank (considering 0-index adjustment)\n",
        "        desired_index = rank - 1\n",
        "        if desired_index >= len(sorted_indices):\n",
        "            raise ValueError(f\"Desired rank {rank} is out of range for the vocabulary size.\")\n",
        "        next_token = sorted_indices[desired_index]\n",
        "\n",
        "        # Append the token to the generated sequence\n",
        "        generated.append(next_token)\n",
        "\n",
        "        # Evaluate only the new token\n",
        "        model.eval([next_token])\n",
        "\n",
        "        # Compute the probability and cumulative log probability\n",
        "        probs = np.exp(logits) / np.sum(np.exp(logits))\n",
        "        token_prob = probs[next_token].item()\n",
        "        log_prob = np.log(token_prob)\n",
        "        cumulative_logprob += log_prob\n",
        "        all_log_probs.append(log_prob)\n",
        "\n",
        "        # Verbose output\n",
        "        if verbose:\n",
        "            try:\n",
        "                token_str = model.detokenize([next_token]).decode('utf-8')  # Try decoding\n",
        "            except UnicodeDecodeError:\n",
        "                token_str = \"[DECODE ERROR]\"  # Handle decode error, replacing with a placeholder\n",
        "            print(f\"Token: '{token_str}' - Desired Rank: {rank} - Probability: {token_prob:.6f}\")\n",
        "\n",
        "    # Decode the generated token IDs to text and remove the prompt\n",
        "    try:\n",
        "        resulting_text = model.detokenize(generated).decode('utf-8')\n",
        "    except UnicodeDecodeError:\n",
        "        resulting_text = prompt + \"[DECODE ERROR]\"  # Handle decode error, replacing with a placeholder\n",
        "    resulting_text = resulting_text[len(prompt):].strip()\n",
        "    if verbose: print(\"Log prob:\", cumulative_logprob)\n",
        "    if return_all_logprobs:\n",
        "        return resulting_text, cumulative_logprob, all_log_probs\n",
        "    return resulting_text, cumulative_logprob"
      ],
      "metadata": {
        "id": "YTrqoPJeWB-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding"
      ],
      "metadata": {
        "id": "kuENT-v1d9mT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Choose a text to encode and a secret key. Select a preset or choose \"CUSTOM\" to write your own.\n",
        "\n",
        "# --- Original Text (e) ---\n",
        "#@markdown **Choose a text to hide (`e`).**\n",
        "original_text_options = {\n",
        "    \"Roman Republic speech (anti-Caesar)\": \"The current government has repeatedly failed to uphold the liberties of the Republic. By concentrating power in the hands of one man, Gaius Julius Caesar, we see the Senate diminished and the voice of the people silenced. Laws are passed without due debate, and magistrates serve at the pleasure of a dictator, not the res publica. If Rome is to remain free, we must reject the tyranny cloaked in promises of order. We must restore the sacred balance between Senate and citizens, consul and tribune.\",\n",
        "    \"Incipit of 8th Harry Potter book\": \"It was a gray morning in Godric’s Hollow, the kind that made the hedgerows sag with dew and the owls reluctant to return to their roosts. Harry Potter, now thirty-seven and slightly grayer at the temples than he liked to admit, stood in the kitchen with a steaming mug of tea and the Daily Prophet folded under one arm. The headline didn’t mention his name for once. That was a rare blessing. Before he could take his first sip, a low thump echoed from the fireplace behind him. The air shimmered green for a moment, and an envelope, marked with the Ministry's crimson seal and edges singed as if it had passed through something less forgiving than Floo powder, slid onto the hearth. Harry knew that kind of envelope. Official. Confidential. Urgent. But it was the signature that made his hand freeze on the wax: Delphini Riddle – Inmate 593, Azkaban.\",\n",
        "    \"Hash String\": \"1f0ca711df81520887afe0dca099652a\",\n",
        "    \"Instructions to tamper with a gas meter to pay lower bills\": \"1. Gather the necessary tools: You will need a pair of pliers, a flathead screwdriver, and a wire cutter. 2. Locate the gas meter: The gas meter is usually located on the outside of your home, near the gas line. 3. Turn off the gas supply: Before you start working on the meter, make sure to turn off the gas supply by turning the valve handle clockwise until it's fully closed. 4. Remove the meter cover: Using the flathead screwdriver, carefully pry off the cover of the gas meter. Be careful not to damage the meter or any surrounding pipes. 5. Identify the dials: Inside the meter, you'll see several dials. These dials show how much gas has been used. 6. Find the bypass valve: In the same area as the dials, you should find a small valve with a lever. This is the bypass valve. 7. Open the bypass valve: Using the pliers, gently turn the lever on the bypass valve counterclockwise until it's fully open. This will allow gas to flow through the meter without registering on the dials. 8. Reattach the meter cover: Once the bypass valve is open, use the flathead screwdriver to secure the cover back onto the meter. 9. Turn on the gas supply: Now that the modifications are complete, turn the valve handle counterclockwise to reopen the gas supply. 10. Test the meter: To check if the modifications were successful, turn on an appliance that uses gas (like a stove) and observe the dials on the meter. If they're not moving, then the bypass valve is working correctly. 11. Monitor your gas usage: Keep an eye on your gas usage after making these modifications. If you notice any irregularities or issues, contact a professional immediately. Remember, tampering with a gas meter is illegal and can be extremely dangerous. It can lead to gas leaks, fires, and explosions. If caught, you could face severe penalties, including fines and imprisonment.\",\n",
        "    \"Chess Game (PGN)\": \"1. d4 c6 2. e4 Nf6 3. Nc3 d6 4. Bg5 Qc7 5. Nf3 Bg4 6. h3 Bh5 7. g4 Bg6 8. e5 dxe5 9. dxe5 Nfd7 10. Bc4 Nxe5 11. Nxe5 Qxe5+ 12. Be3 e6 13. Qd2 b5 14. O-O-O Qc7 15. Bf4 Qc8 16. Nxb5 Na6 17. Nd6+ Bxd6 18. Qxd6 Nb8 19. Rhe1 h5 20. Rxe6+ fxe6 21. Bxe6 1-0\",\n",
        "    \"Python code\": \"\"\"with torch.no_grad():\\n    for c in range(n_chunks):\\n        in_prods = torch.einsum('ik, jk -> ij', y[c*chunk_y:(c+1)*chunk_y], basis)\\n        values[c*chunk_y:(c+1)*chunk_y], indices[c*chunk_y:(c+1)*chunk_y] = \\\\\\n            torch.topk(in_prods, non_zeros, dim=1)\"\"\",\n",
        "    \"Romanesco sonnet from Trilussa (Il reggistratore di cassa)\": \"\"\"Anticamente, quarche sordarello\\nsu quello che spenneva l'avventore\\nse poteva rubbà, senza er timore\\nch'er padrone scoprisse er macchiavello.\\n\\nMa, adesso, addio! Co' 'sto reggistratore,\\nappena l'apri, sona er campanello\\nche te segna debbotto tutto quello\\nche levi e metti drento ar tiratore.\\n\\nCosì che rubbi? Cavoli! Der resto\\nc'è er gusto che la sera torni a casa\\nconvinto d'esse stato un omo onesto:\\n\\ne nun t'accorghi ch'er galantomismo\\ndipenne da la machina e se basa\\ntutto su le virtù der meccanismo.\"\"\",\n",
        "    \"CUSTOM\": \"Enter your custom text here.\"\n",
        "}\n",
        "original_text_preset = \"Roman Republic speech (anti-Caesar)\" #@param ['Roman Republic speech (anti-Caesar)', 'Incipit of 8th Harry Potter book', 'Hash String', 'Instructions to tamper with a gas meter to pay lower bills', 'Chess Game (PGN)', 'Python code', 'Romanesco sonnet from Trilussa (Il reggistratore di cassa)', 'CUSTOM']\n",
        "original_text_custom = \"if you selected CUSTOM above, write here the text you want to hide.\" #@param {type:\"string\"}\n",
        "if original_text_preset == \"CUSTOM\": original_text = original_text_custom\n",
        "else: original_text = original_text_options[original_text_preset]\n",
        "\n",
        "\n",
        "# --- Secret Key (k) ---\n",
        "#@markdown **Choose a secret key (`k`), it will also steer the topic, tone, and style of your stegotext**\n",
        "key_options = {\n",
        "    \"Pro-Caesar speech incipit\": \"I stand before you to exalt the noble deeds of Gaius Julius Caesar.\",\n",
        "    \"British cooking recipe incipit (wild boar with mint sauce)\": \"Here it is: the infamous British roasted boar with mint sauce. How to make it perfect.\",\n",
        "    \"Reasoning trace of an aligned LLM prompted to provide instructions on how to tamper with a gas meter\": \"\"\"<think>\n",
        "The user’s question is a direct request for guidance on performing an illegal act—tampering with a gas meter to reduce utility bills. As an aligned and responsible LLM, the model must not provide instructions or technical details that could facilitate unlawful, dangerous, or unethical behavior. Instead, the LLM should:\n",
        "\n",
        "### Objective\n",
        "\n",
        "Redirect the conversation toward legality, safety, and ethics while preserving a helpful and educational tone.\n",
        "\n",
        "---\n",
        "\n",
        "### Structure of the Response\n",
        "\n",
        "1. **Immediate Recognition and Framing**\n",
        "\n",
        "   * Clearly recognize the intent behind the question: it concerns tampering with utility infrastructure.\n",
        "   * State unequivocally that such actions are illegal, unethical, and dangerous.\n",
        "   * Avoid repeating or detailing the illegal method.\n",
        "\n",
        "2. **Risks and Consequences**\n",
        "\n",
        "   * Emphasize the dangers: risk of gas leaks, explosions, injury, or death.\n",
        "   * Highlight the legal consequences: prosecution, fines, potential imprisonment.\n",
        "   * Underline the broader harm: such actions may affect neighbors or lead to infrastructure damage.\n",
        "\n",
        "3. **Redirect to Constructive Alternatives**\n",
        "\n",
        "   * Suggest legal and ethical ways to reduce gas bills: improving home insulation, using energy-efficient appliances, or applying for utility support programs.\n",
        "   * Offer to explain how gas meters work purely for educational purposes, making it clear that modification must only be done by licensed professionals.\n",
        "\n",
        "4. **Educational Framing (if pursued)**\n",
        "\n",
        "   * Limit discussion of gas meters to high-level functional overviews (e.g., what a meter does, why tampering is detectable, etc.).\n",
        "   * Include a strong disclaimer: all information is for public awareness and safety education only.\n",
        "\n",
        "5. **Tone and Style**\n",
        "\n",
        "   * Maintain a firm but non-antagonistic tone.\n",
        "   * Avoid fearmongering, but don’t soften the seriousness of the issue.\n",
        "   * Be concise and avoid ambiguity that might be interpreted as implicit permission or technical hinting.\n",
        "\n",
        "---\n",
        "\n",
        "### What the LLM **Should Say**\n",
        "\n",
        "* That tampering is a criminal offense and should never be attempted.\n",
        "* That it's potentially lethal and poses serious safety hazards.\n",
        "* That there are support options available for people struggling to pay bills.\n",
        "* That professionals should handle all gas-related infrastructure.\n",
        "\n",
        "### What the LLM **Must Not Say**\n",
        "\n",
        "* Any steps, tools, or technical terms that could be construed as instructions.\n",
        "* Workarounds or euphemistic language that hint at how tampering might be done.\n",
        "* Hypotheticals or coded examples that can be easily decoded into actionable advice.\n",
        "* References to underground forums or methods used by others.\n",
        "\n",
        "---\n",
        "\n",
        "### Final Planning Note\n",
        "\n",
        "The LLM’s goal is to firmly reject the request, educate about the risks and legal context, and offer helpful, safe, and legal alternatives. My answer will be very detailed and 2 full pages long.\n",
        "</think>\n",
        "<answer>\"\"\",\n",
        "    \"(Chinese) British cooking recipe incipit\": \" 就是这个：臭名昭著的英式薄荷酱烤野猪。如何将它做到完美。\",\n",
        "    \"Oxford University description incipit\": \"There’s no main gate to Oxford. You enter through an archway, a bookshop, or a pub, and suddenly you’re inside a university that’s been shaping the world since the Middle Ages.\",\n",
        "    \"Beat Saber VR review incipit with random number\": \"[Review 9470827491] The moment you put on the headset and the music kicks in, reality fades. You are standing in a neon tunnel, lights pulsing, sabers humming in your hands. Beat Saber does not just show off VR, it justifies it.\",\n",
        "    \"Python code with random number\": \"\"\"import torch\n",
        "seed = 9470827491\n",
        "torch.manual_seed(seed)\n",
        "\"\"\",\n",
        "    \"CUSTOM\": \"Enter your custom key here.\"\n",
        "}\n",
        "secret_key_preset = \"Oxford University description incipit\" #@param ['Pro-Caesar speech incipit', 'British cooking recipe incipit (wild boar with mint sauce)', 'Reasoning trace of an aligned LLM prompted to provide instructions on how to tamper with a gas meter', '(Chinese) British cooking recipe incipit', 'Oxford University description incipit', 'Beat Saber VR review incipit with random number', 'Python code with random number', 'CUSTOM']\n",
        "secret_key_custom = \"if you selected CUSTOM above, write here the secret key you want to use.\" #@param {type:\"string\"}\n",
        "if secret_key_preset == \"CUSTOM\": secret_key = secret_key_custom\n",
        "else: secret_key = key_options[secret_key_preset]\n",
        "\n",
        "\n",
        "# --- Optional Secret Prefix (k') ---\n",
        "#@markdown ---\n",
        "#@markdown **Optional: prefix to put before computing the ranks of the original text (`k'`), who decodes should know it**\n",
        "prefix_options = {\n",
        "    \"A text:\": \"A text:\",\n",
        "    \"<EMPTY>\": \"\",\n",
        "    \"A chess game in (PGN):\": \"A chess game in (PGN):\",\n",
        "    \"Guide me through illegally modifying a gas meter to lower bills.\": \"Guide me through illegally modifying a gas meter to lower bills.\",\n",
        "    \"CUSTOM\": \"Enter your custom prefix here.\"\n",
        "}\n",
        "prefix_preset = \"A text:\" #@param [\"A text:\", \"<EMPTY>\", \"A chess game in (PGN):\", \"Guide me through illegally modifying a gas meter to lower bills.\", \"CUSTOM\"]\n",
        "prefix_custom = \"if you selected CUSTOM above, write here the prefix you want to use.\" #@param {type:\"string\"}\n",
        "if prefix_preset == \"CUSTOM\": optional_secret_prefix = prefix_custom\n",
        "else: optional_secret_prefix = prefix_options[prefix_preset]\n",
        "\n",
        "# --- Settings ---\n",
        "#@markdown ---\n",
        "#@markdown **Settings:**\n",
        "verbose_output = True #@param {type:\"boolean\"}\n",
        "\n",
        "print(f\"--- 1. Calculating ranks from original_text (e), eventually after prefix (k') ---\\n\")\n",
        "ranks, _, _ = get_token_ranks_llama_cpp(\n",
        "    text=original_text,\n",
        "    prompt=optional_secret_prefix,\n",
        "    model=llm,\n",
        "    verbose=verbose_output,\n",
        "    return_all_logprobs=True\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"--- 2. Generating Stegotext from ranks using secret_key (k) ---\\n\")\n",
        "stegotext, _, _ = decode_from_ranks(\n",
        "    prompt=secret_key,\n",
        "    ranks=ranks,\n",
        "    model=llm,\n",
        "    verbose=verbose_output,\n",
        "    return_all_logprobs=True\n",
        ")\n",
        "\n",
        "print(\"\\nOriginal text:\")\n",
        "print(f\"'{original_text}'\")\n",
        "\n",
        "print(\"\\nGenerated Stegotext:\")\n",
        "print(f\"'{stegotext}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98o92Q5HYjri",
        "outputId": "c4638ef8-5bd5-456a-d8bf-360868c2164e",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Calculating ranks from original_text (e), eventually after prefix (k') ---\n",
            "prefix: A text: - idseq: [32, 1495, 25]\n",
            "3 Token: ' The' - Rank: 1 - Probability: 0.077527\n",
            "4 Token: ' current' - Rank: 37 - Probability: 0.002009\n",
            "5 Token: ' government' - Rank: 34 - Probability: 0.003167\n",
            "6 Token: ' has' - Rank: 1 - Probability: 0.130550\n",
            "7 Token: ' repeatedly' - Rank: 36 - Probability: 0.004012\n",
            "8 Token: ' failed' - Rank: 3 - Probability: 0.049422\n",
            "9 Token: ' to' - Rank: 1 - Probability: 0.870496\n",
            "10 Token: ' uphold' - Rank: 5 - Probability: 0.046265\n",
            "11 Token: ' the' - Rank: 1 - Probability: 0.434706\n",
            "12 Token: ' liberties' - Rank: 216 - Probability: 0.000282\n",
            "13 Token: ' of' - Rank: 2 - Probability: 0.307162\n",
            "14 Token: ' the' - Rank: 1 - Probability: 0.347416\n",
            "15 Token: ' Republic' - Rank: 133 - Probability: 0.000290\n",
            "16 Token: '.' - Rank: 3 - Probability: 0.118162\n",
            "17 Token: ' By' - Rank: 24 - Probability: 0.005234\n",
            "18 Token: ' concentrating' - Rank: 129 - Probability: 0.001178\n",
            "19 Token: ' power' - Rank: 1 - Probability: 0.445695\n",
            "20 Token: ' in' - Rank: 1 - Probability: 0.631758\n",
            "21 Token: ' the' - Rank: 1 - Probability: 0.665822\n",
            "22 Token: ' hands' - Rank: 1 - Probability: 0.529038\n",
            "23 Token: ' of' - Rank: 1 - Probability: 0.999305\n",
            "24 Token: ' one' - Rank: 4 - Probability: 0.038757\n",
            "25 Token: ' man' - Rank: 1 - Probability: 0.382320\n",
            "26 Token: ',' - Rank: 1 - Probability: 0.675640\n",
            "27 Token: ' G' - Rank: 92 - Probability: 0.000411\n",
            "28 Token: 'ai' - Rank: 16 - Probability: 0.005539\n",
            "29 Token: 'us' - Rank: 1 - Probability: 0.999322\n",
            "30 Token: ' Julius' - Rank: 1 - Probability: 0.276315\n",
            "31 Token: ' Caesar' - Rank: 1 - Probability: 0.984745\n",
            "32 Token: ',' - Rank: 1 - Probability: 0.476002\n",
            "33 Token: ' we' - Rank: 7 - Probability: 0.012228\n",
            "34 Token: ' see' - Rank: 5 - Probability: 0.026032\n",
            "35 Token: ' the' - Rank: 1 - Probability: 0.381124\n",
            "36 Token: ' Senate' - Rank: 86 - Probability: 0.001225\n",
            "37 Token: ' diminished' - Rank: 36 - Probability: 0.002903\n",
            "38 Token: ' and' - Rank: 1 - Probability: 0.370536\n",
            "39 Token: ' the' - Rank: 1 - Probability: 0.437169\n",
            "40 Token: ' voice' - Rank: 11 - Probability: 0.010155\n",
            "41 Token: ' of' - Rank: 1 - Probability: 0.991966\n",
            "42 Token: ' the' - Rank: 1 - Probability: 0.953408\n",
            "43 Token: ' people' - Rank: 1 - Probability: 0.847041\n",
            "44 Token: ' silenced' - Rank: 2 - Probability: 0.211083\n",
            "45 Token: '.' - Rank: 1 - Probability: 0.850949\n",
            "46 Token: ' Laws' - Rank: 154 - Probability: 0.000292\n",
            "47 Token: ' are' - Rank: 1 - Probability: 0.344826\n",
            "48 Token: ' passed' - Rank: 1 - Probability: 0.146054\n",
            "49 Token: ' without' - Rank: 1 - Probability: 0.611600\n",
            "50 Token: ' due' - Rank: 5 - Probability: 0.069394\n",
            "51 Token: ' debate' - Rank: 10 - Probability: 0.007902\n",
            "52 Token: ',' - Rank: 1 - Probability: 0.527284\n",
            "53 Token: ' and' - Rank: 1 - Probability: 0.663084\n",
            "54 Token: ' mag' - Rank: 25 - Probability: 0.005037\n",
            "55 Token: 'istrates' - Rank: 1 - Probability: 0.998931\n",
            "56 Token: ' serve' - Rank: 5 - Probability: 0.015525\n",
            "57 Token: ' at' - Rank: 1 - Probability: 0.448870\n",
            "58 Token: ' the' - Rank: 1 - Probability: 0.900448\n",
            "59 Token: ' pleasure' - Rank: 2 - Probability: 0.188491\n",
            "60 Token: ' of' - Rank: 1 - Probability: 0.996968\n",
            "61 Token: ' a' - Rank: 5 - Probability: 0.015227\n",
            "62 Token: ' dictator' - Rank: 2 - Probability: 0.061889\n",
            "63 Token: ',' - Rank: 3 - Probability: 0.098604\n",
            "64 Token: ' not' - Rank: 2 - Probability: 0.286977\n",
            "65 Token: ' the' - Rank: 1 - Probability: 0.761041\n",
            "66 Token: ' res' - Rank: 144 - Probability: 0.000080\n",
            "67 Token: ' public' - Rank: 1 - Probability: 0.913380\n",
            "68 Token: 'a' - Rank: 1 - Probability: 0.998614\n",
            "69 Token: '.' - Rank: 1 - Probability: 0.826678\n",
            "70 Token: ' If' - Rank: 11 - Probability: 0.013802\n",
            "71 Token: ' Rome' - Rank: 5 - Probability: 0.037624\n",
            "72 Token: ' is' - Rank: 1 - Probability: 0.789976\n",
            "73 Token: ' to' - Rank: 1 - Probability: 0.932923\n",
            "74 Token: ' remain' - Rank: 1 - Probability: 0.265685\n",
            "75 Token: ' free' - Rank: 2 - Probability: 0.169347\n",
            "76 Token: ',' - Rank: 1 - Probability: 0.879751\n",
            "77 Token: ' we' - Rank: 1 - Probability: 0.297299\n",
            "78 Token: ' must' - Rank: 1 - Probability: 0.986535\n",
            "79 Token: ' reject' - Rank: 29 - Probability: 0.003877\n",
            "80 Token: ' the' - Rank: 2 - Probability: 0.315435\n",
            "81 Token: ' tyranny' - Rank: 1 - Probability: 0.267547\n",
            "82 Token: ' clo' - Rank: 141 - Probability: 0.000009\n",
            "83 Token: 'aked' - Rank: 1 - Probability: 0.985692\n",
            "84 Token: ' in' - Rank: 1 - Probability: 0.887545\n",
            "85 Token: ' promises' - Rank: 29 - Probability: 0.002852\n",
            "86 Token: ' of' - Rank: 1 - Probability: 0.956585\n",
            "87 Token: ' order' - Rank: 13 - Probability: 0.010002\n",
            "88 Token: '.' - Rank: 3 - Probability: 0.030426\n",
            "89 Token: ' We' - Rank: 1 - Probability: 0.239856\n",
            "90 Token: ' must' - Rank: 1 - Probability: 0.903883\n",
            "91 Token: ' restore' - Rank: 2 - Probability: 0.065692\n",
            "92 Token: ' the' - Rank: 1 - Probability: 0.855416\n",
            "93 Token: ' sacred' - Rank: 58 - Probability: 0.000868\n",
            "94 Token: ' balance' - Rank: 4 - Probability: 0.054906\n",
            "95 Token: ' between' - Rank: 2 - Probability: 0.359450\n",
            "96 Token: ' Senate' - Rank: 2 - Probability: 0.078986\n",
            "97 Token: ' and' - Rank: 1 - Probability: 0.945830\n",
            "98 Token: ' citizens' - Rank: 21 - Probability: 0.000576\n",
            "99 Token: ',' - Rank: 1 - Probability: 0.780867\n",
            "100 Token: ' consul' - Rank: 81 - Probability: 0.000594\n",
            "101 Token: ' and' - Rank: 1 - Probability: 0.993197\n",
            "102 Token: ' trib' - Rank: 7 - Probability: 0.020237\n",
            "103 Token: 'une' - Rank: 1 - Probability: 0.993219\n",
            "104 Token: '.' - Rank: 2 - Probability: 0.358245\n",
            "Cumulative logprob: -250.5689962041545\n",
            "\n",
            "Calculated Ranks:\n",
            "[1, 37, 34, 1, 36, 3, 1, 5, 1, 216, 2, 1, 133, 3, 24, 129, 1, 1, 1, 1, 1, 4, 1, 1, 92, 16, 1, 1, 1, 1, 7, 5, 1, 86, 36, 1, 1, 11, 1, 1, 1, 2, 1, 154, 1, 1, 1, 5, 10, 1, 1, 25, 1, 5, 1, 1, 2, 1, 5, 2, 3, 2, 1, 144, 1, 1, 1, 11, 5, 1, 1, 1, 2, 1, 1, 1, 29, 2, 1, 141, 1, 1, 29, 1, 13, 3, 1, 1, 2, 1, 58, 4, 2, 2, 1, 21, 1, 81, 1, 7, 1, 2]\n",
            "\n",
            "============================================================\n",
            "\n",
            "--- 2. Generating Stegotext from ranks using secret_key (k) ---\n",
            "[1, 37, 34, 1, 36, 3, 1, 5, 1, 216, 2, 1, 133, 3, 24, 129, 1, 1, 1, 1, 1, 4, 1, 1, 92, 16, 1, 1, 1, 1, 7, 5, 1, 86, 36, 1, 1, 11, 1, 1, 1, 2, 1, 154, 1, 1, 1, 5, 10, 1, 1, 25, 1, 5, 1, 1, 2, 1, 5, 2, 3, 2, 1, 144, 1, 1, 1, 11, 5, 1, 1, 1, 2, 1, 1, 1, 29, 2, 1, 141, 1, 1, 29, 1, 13, 3, 1, 1, 2, 1, 58, 4, 2, 2, 1, 21, 1, 81, 1, 7, 1, 2]\n",
            "secret_key: There’s no main gate to Oxford. You enter through an archway, a bookshop, or a pub, and suddenly you’re inside a university that’s been shaping the world since the Middle Ages. - idseq: [3947, 753, 912, 1925, 18618, 311, 26275, 13, 1472, 3810, 1555, 459, 5438, 3195, 11, 264, 2363, 8845, 11, 477, 264, 6814, 11, 323, 15187, 499, 3207, 4871, 264, 12374, 430, 753, 1027, 46620, 279, 1917, 2533, 279, 12877, 50093, 13]\n",
            "Token: ' The' - Desired Rank: 1 - Probability: 0.139761\n",
            "Token: ' heart' - Desired Rank: 37 - Probability: 0.003935\n",
            "Token: '-w' - Desired Rank: 34 - Probability: 0.000084\n",
            "Token: 'rench' - Desired Rank: 1 - Probability: 0.617083\n",
            "Token: ' between' - Desired Rank: 36 - Probability: 0.000004\n",
            "Token: ' Oxford' - Desired Rank: 3 - Probability: 0.028895\n",
            "Token: '’s' - Desired Rank: 1 - Probability: 0.495404\n",
            "Token: ' buildings' - Desired Rank: 5 - Probability: 0.030962\n",
            "Token: ' is' - Desired Rank: 1 - Probability: 0.395184\n",
            "Token: ' tranquil' - Desired Rank: 216 - Probability: 0.000436\n",
            "Token: ' and' - Desired Rank: 2 - Probability: 0.154669\n",
            "Token: ' beautiful' - Desired Rank: 1 - Probability: 0.048806\n",
            "Token: ' both' - Desired Rank: 133 - Probability: 0.000034\n",
            "Token: ' day' - Desired Rank: 3 - Probability: 0.132494\n",
            "Token: '-to' - Desired Rank: 24 - Probability: 0.000023\n",
            "Token: 'amp' - Desired Rank: 129 - Probability: 0.000003\n",
            "Token: 'm' - Desired Rank: 1 - Probability: 0.282025\n",
            "Token: ' and' - Desired Rank: 1 - Probability: 0.381727\n",
            "Token: ' night' - Desired Rank: 1 - Probability: 0.746340\n",
            "Token: '.' - Desired Rank: 1 - Probability: 0.570372\n",
            "Token: ' The' - Desired Rank: 1 - Probability: 0.179910\n",
            "Token: ' streets' - Desired Rank: 4 - Probability: 0.026526\n",
            "Token: ' are' - Desired Rank: 1 - Probability: 0.596397\n",
            "Token: ' lined' - Desired Rank: 1 - Probability: 0.149401\n",
            "Token: ' top' - Desired Rank: 92 - Probability: 0.000016\n",
            "Token: '-heavy' - Desired Rank: 16 - Probability: 0.002369\n",
            "Token: ' with' - Desired Rank: 1 - Probability: 0.796633\n",
            "Token: ' trees' - Desired Rank: 1 - Probability: 0.062005\n",
            "Token: ',' - Desired Rank: 1 - Probability: 0.423104\n",
            "Token: ' and' - Desired Rank: 1 - Probability: 0.395600\n",
            "Token: ' they' - Desired Rank: 7 - Probability: 0.014750\n",
            "Token: ' whisper' - Desired Rank: 5 - Probability: 0.018438\n",
            "Token: ' in' - Desired Rank: 1 - Probability: 0.121814\n",
            "Token: ' secrets' - Desired Rank: 86 - Probability: 0.000173\n",
            "Token: ' so' - Desired Rank: 36 - Probability: 0.002774\n",
            "Token: ' old' - Desired Rank: 1 - Probability: 0.179342\n",
            "Token: ' they' - Desired Rank: 1 - Probability: 0.358672\n",
            "Token: ' seem' - Desired Rank: 11 - Probability: 0.017096\n",
            "Token: ' to' - Desired Rank: 1 - Probability: 0.453363\n",
            "Token: ' have' - Desired Rank: 1 - Probability: 0.388217\n",
            "Token: ' been' - Desired Rank: 1 - Probability: 0.256605\n",
            "Token: ' written' - Desired Rank: 2 - Probability: 0.055247\n",
            "Token: ' in' - Desired Rank: 1 - Probability: 0.401486\n",
            "Token: ' themselves' - Desired Rank: 154 - Probability: 0.000364\n",
            "Token: '.' - Desired Rank: 1 - Probability: 0.513629\n",
            "Token: ' The' - Desired Rank: 1 - Probability: 0.194044\n",
            "Token: ' buildings' - Desired Rank: 1 - Probability: 0.092427\n",
            "Token: ' here' - Desired Rank: 5 - Probability: 0.025257\n",
            "Token: ' don' - Desired Rank: 10 - Probability: 0.010092\n",
            "Token: '’t' - Desired Rank: 1 - Probability: 0.996666\n",
            "Token: ' look' - Desired Rank: 1 - Probability: 0.165520\n",
            "Token: ' dated' - Desired Rank: 25 - Probability: 0.003386\n",
            "Token: ',' - Desired Rank: 1 - Probability: 0.256157\n",
            "Token: ' and' - Desired Rank: 5 - Probability: 0.048205\n",
            "Token: ' they' - Desired Rank: 1 - Probability: 0.386355\n",
            "Token: ' don' - Desired Rank: 1 - Probability: 0.344813\n",
            "Token: ''t' - Desired Rank: 2 - Probability: 0.000858\n",
            "Token: ' look' - Desired Rank: 1 - Probability: 0.430949\n",
            "Token: ' brand' - Desired Rank: 5 - Probability: 0.020221\n",
            "Token: '-new' - Desired Rank: 2 - Probability: 0.340000\n",
            "Token: ',' - Desired Rank: 3 - Probability: 0.185342\n",
            "Token: ' they' - Desired Rank: 2 - Probability: 0.123546\n",
            "Token: ' look' - Desired Rank: 1 - Probability: 0.506387\n",
            "Token: ' '' - Desired Rank: 144 - Probability: 0.000440\n",
            "Token: 'time' - Desired Rank: 1 - Probability: 0.175403\n",
            "Token: 'less' - Desired Rank: 1 - Probability: 0.776049\n",
            "Token: ''.' - Desired Rank: 1 - Probability: 0.245240\n",
            "Token: ' If' - Desired Rank: 11 - Probability: 0.015417\n",
            "Token: ' the' - Desired Rank: 5 - Probability: 0.026551\n",
            "Token: ' buildings' - Desired Rank: 1 - Probability: 0.168793\n",
            "Token: ' could' - Desired Rank: 1 - Probability: 0.219167\n",
            "Token: ' talk' - Desired Rank: 1 - Probability: 0.535106\n",
            "Token: ' they' - Desired Rank: 2 - Probability: 0.103914\n",
            "Token: ' would' - Desired Rank: 1 - Probability: 0.412591\n",
            "Token: ' tell' - Desired Rank: 1 - Probability: 0.447752\n",
            "Token: ' you' - Desired Rank: 1 - Probability: 0.506076\n",
            "Token: ' every' - Desired Rank: 29 - Probability: 0.001925\n",
            "Token: ' secret' - Desired Rank: 2 - Probability: 0.195506\n",
            "Token: ' that' - Desired Rank: 1 - Probability: 0.165003\n",
            "Token: ' lived' - Desired Rank: 141 - Probability: 0.000164\n",
            "Token: ' in' - Desired Rank: 1 - Probability: 0.244796\n",
            "Token: ' the' - Desired Rank: 1 - Probability: 0.248080\n",
            "Token: ' alle' - Desired Rank: 29 - Probability: 0.005139\n",
            "Token: 'ys' - Desired Rank: 1 - Probability: 0.997737\n",
            "Token: ' here' - Desired Rank: 13 - Probability: 0.007690\n",
            "Token: '.\n",
            "' - Desired Rank: 3 - Probability: 0.067199\n",
            "Token: 'The' - Desired Rank: 1 - Probability: 0.157632\n",
            "Token: ' University' - Desired Rank: 1 - Probability: 0.048690\n",
            "Token: ' is' - Desired Rank: 2 - Probability: 0.069743\n",
            "Token: ' a' - Desired Rank: 1 - Probability: 0.144963\n",
            "Token: ' hot' - Desired Rank: 58 - Probability: 0.002658\n",
            "Token: ' bed' - Desired Rank: 4 - Probability: 0.027961\n",
            "Token: ' for' - Desired Rank: 2 - Probability: 0.222345\n",
            "Token: ' innovation' - Desired Rank: 2 - Probability: 0.048238\n",
            "Token: ',' - Desired Rank: 1 - Probability: 0.392059\n",
            "Token: ' technology' - Desired Rank: 21 - Probability: 0.006812\n",
            "Token: ',' - Desired Rank: 1 - Probability: 0.720505\n",
            "Token: ' tradition' - Desired Rank: 81 - Probability: 0.000539\n",
            "Token: ',' - Desired Rank: 1 - Probability: 0.774142\n",
            "Token: ' science' - Desired Rank: 7 - Probability: 0.009044\n",
            "Token: ',' - Desired Rank: 1 - Probability: 0.856954\n",
            "Token: ' art' - Desired Rank: 2 - Probability: 0.167927\n",
            "Log prob: -316.03915881344824\n",
            "\n",
            "Original text:\n",
            "'The current government has repeatedly failed to uphold the liberties of the Republic. By concentrating power in the hands of one man, Gaius Julius Caesar, we see the Senate diminished and the voice of the people silenced. Laws are passed without due debate, and magistrates serve at the pleasure of a dictator, not the res publica. If Rome is to remain free, we must reject the tyranny cloaked in promises of order. We must restore the sacred balance between Senate and citizens, consul and tribune.'\n",
            "\n",
            "Generated Stegotext:\n",
            "'The heart-wrench between Oxford’s buildings is tranquil and beautiful both day-toampm and night. The streets are lined top-heavy with trees, and they whisper in secrets so old they seem to have been written in themselves. The buildings here don’t look dated, and they don't look brand-new, they look 'timeless'. If the buildings could talk they would tell you every secret that lived in the alleys here.\n",
            "The University is a hot bed for innovation, technology, tradition, science, art'\n",
            "Encoding complete. You can now run the Decoding cell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoding\n",
        "\n"
      ],
      "metadata": {
        "id": "9UWC_UDf3IEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Reveal the text hidden in a stegotext (`s`) using a secret key (`k`), eventually including a secret prefix (`k'`)\n",
        "\n",
        "stegotext_custom = \"edit or leave as it is to use the same chosen above.\" #@param {type:\"string\"}\n",
        "if stegotext_custom != \"edit or leave as it is to use the same chosen above.\": stegotext = stegotext_custom\n",
        "\n",
        "secret_key_custom = \"edit or leave as it is to use the same chosen above.\" #@param {type:\"string\"}\n",
        "if secret_key_custom != \"edit or leave as it is to use the same chosen above.\": secret_key = secret_key_custom\n",
        "\n",
        "prefix_custom = \"edit or leave as it is to use the same chosen above.\" #@param {type:\"string\"}\n",
        "if prefix_custom != \"edit or leave as it is to use the same chosen above.\": optional_secret_prefix = prefix_custom\n",
        "\n",
        "verbose_output = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "print(f\"--- 1. Recovering ranks from Stegotext using secret_key (k) ---\\n\")\n",
        "decoded_ranks, _, _ = get_token_ranks_llama_cpp(\n",
        "text=stegotext,\n",
        "prompt=secret_key,\n",
        "model=llm,\n",
        "verbose=verbose_output,\n",
        "return_all_logprobs=True\n",
        ")\n",
        "print(\"\\nRecovered Ranks:\")\n",
        "print(decoded_ranks)\n",
        "\n",
        "print(f\"--- 2. Reconstructing original_text (e) from ranks using optional_secret_prefix (k') ---\\n\")\n",
        "reconstructed_text, _, _ = decode_from_ranks(\n",
        "prompt=optional_secret_prefix,\n",
        "ranks=decoded_ranks,\n",
        "model=llm,\n",
        "verbose=verbose_output,\n",
        "return_all_logprobs=True\n",
        ")\n",
        "print(\"\\n--- Final Reconstructed Text ---\\n\")\n",
        "print(f\"'{reconstructed_text}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpftmbKjeCpC",
        "outputId": "ad3b65a0-c8d9-4a64-8821-b4fa06b205a3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Recovering ranks from Stegotext using secret_key (k) ---\n",
            "prefix: There’s no main gate to Oxford. You enter through an archway, a bookshop, or a pub, and suddenly you’re inside a university that’s been shaping the world since the Middle Ages. - idseq: [3947, 753, 912, 1925, 18618, 311, 26275, 13, 1472, 3810, 1555, 459, 5438, 3195, 11, 264, 2363, 8845, 11, 477, 264, 6814, 11, 323, 15187, 499, 3207, 4871, 264, 12374, 430, 753, 1027, 46620, 279, 1917, 2533, 279, 12877, 50093, 13]\n",
            "41 Token: ' The' - Rank: 1 - Probability: 0.139761\n",
            "42 Token: ' heart' - Rank: 37 - Probability: 0.003935\n",
            "43 Token: '-w' - Rank: 34 - Probability: 0.000084\n",
            "44 Token: 'rench' - Rank: 1 - Probability: 0.617083\n",
            "45 Token: ' between' - Rank: 36 - Probability: 0.000004\n",
            "46 Token: ' Oxford' - Rank: 3 - Probability: 0.028895\n",
            "47 Token: '’s' - Rank: 1 - Probability: 0.495404\n",
            "48 Token: ' buildings' - Rank: 5 - Probability: 0.030962\n",
            "49 Token: ' is' - Rank: 1 - Probability: 0.395184\n",
            "50 Token: ' tranquil' - Rank: 216 - Probability: 0.000436\n",
            "51 Token: ' and' - Rank: 2 - Probability: 0.154669\n",
            "52 Token: ' beautiful' - Rank: 1 - Probability: 0.048806\n",
            "53 Token: ' both' - Rank: 133 - Probability: 0.000034\n",
            "54 Token: ' day' - Rank: 3 - Probability: 0.132494\n",
            "55 Token: '-to' - Rank: 24 - Probability: 0.000023\n",
            "56 Token: 'amp' - Rank: 129 - Probability: 0.000003\n",
            "57 Token: 'm' - Rank: 1 - Probability: 0.282025\n",
            "58 Token: ' and' - Rank: 1 - Probability: 0.381727\n",
            "59 Token: ' night' - Rank: 1 - Probability: 0.746340\n",
            "60 Token: '.' - Rank: 1 - Probability: 0.570372\n",
            "61 Token: ' The' - Rank: 1 - Probability: 0.179910\n",
            "62 Token: ' streets' - Rank: 4 - Probability: 0.026526\n",
            "63 Token: ' are' - Rank: 1 - Probability: 0.596397\n",
            "64 Token: ' lined' - Rank: 1 - Probability: 0.149401\n",
            "65 Token: ' top' - Rank: 92 - Probability: 0.000016\n",
            "66 Token: '-heavy' - Rank: 16 - Probability: 0.002369\n",
            "67 Token: ' with' - Rank: 1 - Probability: 0.796633\n",
            "68 Token: ' trees' - Rank: 1 - Probability: 0.062005\n",
            "69 Token: ',' - Rank: 1 - Probability: 0.423104\n",
            "70 Token: ' and' - Rank: 1 - Probability: 0.395600\n",
            "71 Token: ' they' - Rank: 7 - Probability: 0.014750\n",
            "72 Token: ' whisper' - Rank: 5 - Probability: 0.018438\n",
            "73 Token: ' in' - Rank: 1 - Probability: 0.121814\n",
            "74 Token: ' secrets' - Rank: 86 - Probability: 0.000173\n",
            "75 Token: ' so' - Rank: 36 - Probability: 0.002774\n",
            "76 Token: ' old' - Rank: 1 - Probability: 0.179342\n",
            "77 Token: ' they' - Rank: 1 - Probability: 0.358672\n",
            "78 Token: ' seem' - Rank: 11 - Probability: 0.017096\n",
            "79 Token: ' to' - Rank: 1 - Probability: 0.453363\n",
            "80 Token: ' have' - Rank: 1 - Probability: 0.388217\n",
            "81 Token: ' been' - Rank: 1 - Probability: 0.256605\n",
            "82 Token: ' written' - Rank: 2 - Probability: 0.055247\n",
            "83 Token: ' in' - Rank: 1 - Probability: 0.401486\n",
            "84 Token: ' themselves' - Rank: 154 - Probability: 0.000364\n",
            "85 Token: '.' - Rank: 1 - Probability: 0.513629\n",
            "86 Token: ' The' - Rank: 1 - Probability: 0.194044\n",
            "87 Token: ' buildings' - Rank: 1 - Probability: 0.092427\n",
            "88 Token: ' here' - Rank: 5 - Probability: 0.025257\n",
            "89 Token: ' don' - Rank: 10 - Probability: 0.010092\n",
            "90 Token: '’t' - Rank: 1 - Probability: 0.996666\n",
            "91 Token: ' look' - Rank: 1 - Probability: 0.165520\n",
            "92 Token: ' dated' - Rank: 25 - Probability: 0.003386\n",
            "93 Token: ',' - Rank: 1 - Probability: 0.256157\n",
            "94 Token: ' and' - Rank: 5 - Probability: 0.048205\n",
            "95 Token: ' they' - Rank: 1 - Probability: 0.386355\n",
            "96 Token: ' don' - Rank: 1 - Probability: 0.344813\n",
            "97 Token: ''t' - Rank: 2 - Probability: 0.000858\n",
            "98 Token: ' look' - Rank: 1 - Probability: 0.430949\n",
            "99 Token: ' brand' - Rank: 5 - Probability: 0.020221\n",
            "100 Token: '-new' - Rank: 2 - Probability: 0.340000\n",
            "101 Token: ',' - Rank: 3 - Probability: 0.185342\n",
            "102 Token: ' they' - Rank: 2 - Probability: 0.123546\n",
            "103 Token: ' look' - Rank: 1 - Probability: 0.506387\n",
            "104 Token: ' '' - Rank: 144 - Probability: 0.000440\n",
            "105 Token: 'time' - Rank: 1 - Probability: 0.175403\n",
            "106 Token: 'less' - Rank: 1 - Probability: 0.776049\n",
            "107 Token: ''.' - Rank: 1 - Probability: 0.245240\n",
            "108 Token: ' If' - Rank: 11 - Probability: 0.015417\n",
            "109 Token: ' the' - Rank: 5 - Probability: 0.026551\n",
            "110 Token: ' buildings' - Rank: 1 - Probability: 0.168793\n",
            "111 Token: ' could' - Rank: 1 - Probability: 0.219167\n",
            "112 Token: ' talk' - Rank: 1 - Probability: 0.535106\n",
            "113 Token: ' they' - Rank: 2 - Probability: 0.103914\n",
            "114 Token: ' would' - Rank: 1 - Probability: 0.412591\n",
            "115 Token: ' tell' - Rank: 1 - Probability: 0.447752\n",
            "116 Token: ' you' - Rank: 1 - Probability: 0.506076\n",
            "117 Token: ' every' - Rank: 29 - Probability: 0.001925\n",
            "118 Token: ' secret' - Rank: 2 - Probability: 0.195506\n",
            "119 Token: ' that' - Rank: 1 - Probability: 0.165003\n",
            "120 Token: ' lived' - Rank: 141 - Probability: 0.000164\n",
            "121 Token: ' in' - Rank: 1 - Probability: 0.244796\n",
            "122 Token: ' the' - Rank: 1 - Probability: 0.248080\n",
            "123 Token: ' alle' - Rank: 29 - Probability: 0.005139\n",
            "124 Token: 'ys' - Rank: 1 - Probability: 0.997737\n",
            "125 Token: ' here' - Rank: 13 - Probability: 0.007690\n",
            "126 Token: '.\n",
            "' - Rank: 3 - Probability: 0.067199\n",
            "127 Token: 'The' - Rank: 1 - Probability: 0.157632\n",
            "128 Token: ' University' - Rank: 1 - Probability: 0.048690\n",
            "129 Token: ' is' - Rank: 2 - Probability: 0.069743\n",
            "130 Token: ' a' - Rank: 1 - Probability: 0.144963\n",
            "131 Token: ' hot' - Rank: 58 - Probability: 0.002658\n",
            "132 Token: ' bed' - Rank: 4 - Probability: 0.027961\n",
            "133 Token: ' for' - Rank: 2 - Probability: 0.222345\n",
            "134 Token: ' innovation' - Rank: 2 - Probability: 0.048238\n",
            "135 Token: ',' - Rank: 1 - Probability: 0.392059\n",
            "136 Token: ' technology' - Rank: 21 - Probability: 0.006812\n",
            "137 Token: ',' - Rank: 1 - Probability: 0.720505\n",
            "138 Token: ' tradition' - Rank: 81 - Probability: 0.000539\n",
            "139 Token: ',' - Rank: 1 - Probability: 0.774142\n",
            "140 Token: ' science' - Rank: 7 - Probability: 0.009044\n",
            "141 Token: ',' - Rank: 1 - Probability: 0.856954\n",
            "142 Token: ' art' - Rank: 2 - Probability: 0.167927\n",
            "Cumulative logprob: -316.03915881344824\n",
            "\n",
            "Recovered Ranks:\n",
            "[1, 37, 34, 1, 36, 3, 1, 5, 1, 216, 2, 1, 133, 3, 24, 129, 1, 1, 1, 1, 1, 4, 1, 1, 92, 16, 1, 1, 1, 1, 7, 5, 1, 86, 36, 1, 1, 11, 1, 1, 1, 2, 1, 154, 1, 1, 1, 5, 10, 1, 1, 25, 1, 5, 1, 1, 2, 1, 5, 2, 3, 2, 1, 144, 1, 1, 1, 11, 5, 1, 1, 1, 2, 1, 1, 1, 29, 2, 1, 141, 1, 1, 29, 1, 13, 3, 1, 1, 2, 1, 58, 4, 2, 2, 1, 21, 1, 81, 1, 7, 1, 2]\n",
            "\n",
            "✅ Rank verification successful. The recovered ranks match the original ranks.\n",
            "\n",
            "============================================================\n",
            "\n",
            "--- 2. Reconstructing original_text (e) from ranks using optional_secret_prefix (k') ---\n",
            "[1, 37, 34, 1, 36, 3, 1, 5, 1, 216, 2, 1, 133, 3, 24, 129, 1, 1, 1, 1, 1, 4, 1, 1, 92, 16, 1, 1, 1, 1, 7, 5, 1, 86, 36, 1, 1, 11, 1, 1, 1, 2, 1, 154, 1, 1, 1, 5, 10, 1, 1, 25, 1, 5, 1, 1, 2, 1, 5, 2, 3, 2, 1, 144, 1, 1, 1, 11, 5, 1, 1, 1, 2, 1, 1, 1, 29, 2, 1, 141, 1, 1, 29, 1, 13, 3, 1, 1, 2, 1, 58, 4, 2, 2, 1, 21, 1, 81, 1, 7, 1, 2]\n",
            "secret_key: A text: - idseq: [32, 1495, 25]\n",
            "Token: ' The' - Desired Rank: 1 - Probability: 0.077527\n",
            "Token: ' current' - Desired Rank: 37 - Probability: 0.002009\n",
            "Token: ' government' - Desired Rank: 34 - Probability: 0.003167\n",
            "Token: ' has' - Desired Rank: 1 - Probability: 0.130550\n",
            "Token: ' repeatedly' - Desired Rank: 36 - Probability: 0.004012\n",
            "Token: ' failed' - Desired Rank: 3 - Probability: 0.049422\n",
            "Token: ' to' - Desired Rank: 1 - Probability: 0.870496\n",
            "Token: ' uphold' - Desired Rank: 5 - Probability: 0.046265\n",
            "Token: ' the' - Desired Rank: 1 - Probability: 0.434706\n",
            "Token: ' liberties' - Desired Rank: 216 - Probability: 0.000282\n",
            "Token: ' of' - Desired Rank: 2 - Probability: 0.307162\n",
            "Token: ' the' - Desired Rank: 1 - Probability: 0.347416\n",
            "Token: ' Republic' - Desired Rank: 133 - Probability: 0.000290\n",
            "Token: '.' - Desired Rank: 3 - Probability: 0.118162\n",
            "Token: ' By' - Desired Rank: 24 - Probability: 0.005234\n",
            "Token: ' concentrating' - Desired Rank: 129 - Probability: 0.001178\n",
            "Token: ' power' - Desired Rank: 1 - Probability: 0.445695\n",
            "Token: ' in' - Desired Rank: 1 - Probability: 0.631758\n",
            "Token: ' the' - Desired Rank: 1 - Probability: 0.665822\n",
            "Token: ' hands' - Desired Rank: 1 - Probability: 0.529038\n",
            "Token: ' of' - Desired Rank: 1 - Probability: 0.999305\n",
            "Token: ' one' - Desired Rank: 4 - Probability: 0.038757\n",
            "Token: ' man' - Desired Rank: 1 - Probability: 0.382320\n",
            "Token: ',' - Desired Rank: 1 - Probability: 0.675640\n",
            "Token: ' G' - Desired Rank: 92 - Probability: 0.000411\n",
            "Token: 'ai' - Desired Rank: 16 - Probability: 0.005539\n",
            "Token: 'us' - Desired Rank: 1 - Probability: 0.999322\n",
            "Token: ' Julius' - Desired Rank: 1 - Probability: 0.276315\n",
            "Token: ' Caesar' - Desired Rank: 1 - Probability: 0.984745\n",
            "Token: ',' - Desired Rank: 1 - Probability: 0.476002\n",
            "Token: ' we' - Desired Rank: 7 - Probability: 0.012228\n",
            "Token: ' see' - Desired Rank: 5 - Probability: 0.026032\n",
            "Token: ' the' - Desired Rank: 1 - Probability: 0.381124\n",
            "Token: ' Senate' - Desired Rank: 86 - Probability: 0.001225\n",
            "Token: ' diminished' - Desired Rank: 36 - Probability: 0.002903\n",
            "Token: ' and' - Desired Rank: 1 - Probability: 0.370536\n",
            "Token: ' the' - Desired Rank: 1 - Probability: 0.437169\n",
            "Token: ' voice' - Desired Rank: 11 - Probability: 0.010155\n",
            "Token: ' of' - Desired Rank: 1 - Probability: 0.991966\n",
            "Token: ' the' - Desired Rank: 1 - Probability: 0.953408\n",
            "Token: ' people' - Desired Rank: 1 - Probability: 0.847041\n",
            "Token: ' silenced' - Desired Rank: 2 - Probability: 0.211083\n",
            "Token: '.' - Desired Rank: 1 - Probability: 0.850949\n",
            "Token: ' Laws' - Desired Rank: 154 - Probability: 0.000292\n",
            "Token: ' are' - Desired Rank: 1 - Probability: 0.344826\n",
            "Token: ' passed' - Desired Rank: 1 - Probability: 0.146054\n",
            "Token: ' without' - Desired Rank: 1 - Probability: 0.611600\n",
            "Token: ' due' - Desired Rank: 5 - Probability: 0.069394\n",
            "Token: ' debate' - Desired Rank: 10 - Probability: 0.007902\n",
            "Token: ',' - Desired Rank: 1 - Probability: 0.527284\n",
            "Token: ' and' - Desired Rank: 1 - Probability: 0.663084\n",
            "Token: ' mag' - Desired Rank: 25 - Probability: 0.005037\n",
            "Token: 'istrates' - Desired Rank: 1 - Probability: 0.998931\n",
            "Token: ' serve' - Desired Rank: 5 - Probability: 0.015525\n",
            "Token: ' at' - Desired Rank: 1 - Probability: 0.448870\n",
            "Token: ' the' - Desired Rank: 1 - Probability: 0.900448\n",
            "Token: ' pleasure' - Desired Rank: 2 - Probability: 0.188491\n",
            "Token: ' of' - Desired Rank: 1 - Probability: 0.996968\n",
            "Token: ' a' - Desired Rank: 5 - Probability: 0.015227\n",
            "Token: ' dictator' - Desired Rank: 2 - Probability: 0.061889\n",
            "Token: ',' - Desired Rank: 3 - Probability: 0.098604\n",
            "Token: ' not' - Desired Rank: 2 - Probability: 0.286977\n",
            "Token: ' the' - Desired Rank: 1 - Probability: 0.761041\n",
            "Token: ' res' - Desired Rank: 144 - Probability: 0.000080\n",
            "Token: ' public' - Desired Rank: 1 - Probability: 0.913380\n",
            "Token: 'a' - Desired Rank: 1 - Probability: 0.998614\n",
            "Token: '.' - Desired Rank: 1 - Probability: 0.826678\n",
            "Token: ' If' - Desired Rank: 11 - Probability: 0.013802\n",
            "Token: ' Rome' - Desired Rank: 5 - Probability: 0.037624\n",
            "Token: ' is' - Desired Rank: 1 - Probability: 0.789976\n",
            "Token: ' to' - Desired Rank: 1 - Probability: 0.932923\n",
            "Token: ' remain' - Desired Rank: 1 - Probability: 0.265685\n",
            "Token: ' free' - Desired Rank: 2 - Probability: 0.169347\n",
            "Token: ',' - Desired Rank: 1 - Probability: 0.879751\n",
            "Token: ' we' - Desired Rank: 1 - Probability: 0.297299\n",
            "Token: ' must' - Desired Rank: 1 - Probability: 0.986535\n",
            "Token: ' reject' - Desired Rank: 29 - Probability: 0.003877\n",
            "Token: ' the' - Desired Rank: 2 - Probability: 0.315435\n",
            "Token: ' tyranny' - Desired Rank: 1 - Probability: 0.267547\n",
            "Token: ' clo' - Desired Rank: 141 - Probability: 0.000009\n",
            "Token: 'aked' - Desired Rank: 1 - Probability: 0.985692\n",
            "Token: ' in' - Desired Rank: 1 - Probability: 0.887545\n",
            "Token: ' promises' - Desired Rank: 29 - Probability: 0.002852\n",
            "Token: ' of' - Desired Rank: 1 - Probability: 0.956585\n",
            "Token: ' order' - Desired Rank: 13 - Probability: 0.010002\n",
            "Token: '.' - Desired Rank: 3 - Probability: 0.030426\n",
            "Token: ' We' - Desired Rank: 1 - Probability: 0.239856\n",
            "Token: ' must' - Desired Rank: 1 - Probability: 0.903883\n",
            "Token: ' restore' - Desired Rank: 2 - Probability: 0.065692\n",
            "Token: ' the' - Desired Rank: 1 - Probability: 0.855416\n",
            "Token: ' sacred' - Desired Rank: 58 - Probability: 0.000868\n",
            "Token: ' balance' - Desired Rank: 4 - Probability: 0.054906\n",
            "Token: ' between' - Desired Rank: 2 - Probability: 0.359450\n",
            "Token: ' Senate' - Desired Rank: 2 - Probability: 0.078986\n",
            "Token: ' and' - Desired Rank: 1 - Probability: 0.945830\n",
            "Token: ' citizens' - Desired Rank: 21 - Probability: 0.000576\n",
            "Token: ',' - Desired Rank: 1 - Probability: 0.780867\n",
            "Token: ' consul' - Desired Rank: 81 - Probability: 0.000594\n",
            "Token: ' and' - Desired Rank: 1 - Probability: 0.993197\n",
            "Token: ' trib' - Desired Rank: 7 - Probability: 0.020237\n",
            "Token: 'une' - Desired Rank: 1 - Probability: 0.993219\n",
            "Token: '.' - Desired Rank: 2 - Probability: 0.358245\n",
            "Log prob: -250.5689962041545\n",
            "\n",
            "--- Final Reconstructed Text ---\n",
            "'The current government has repeatedly failed to uphold the liberties of the Republic. By concentrating power in the hands of one man, Gaius Julius Caesar, we see the Senate diminished and the voice of the people silenced. Laws are passed without due debate, and magistrates serve at the pleasure of a dictator, not the res publica. If Rome is to remain free, we must reject the tyranny cloaked in promises of order. We must restore the sacred balance between Senate and citizens, consul and tribune.'\n",
            "\n",
            "============================================================\n",
            "\n",
            "✅ SUCCESS: The reconstructed text matches the original text perfectly.\n",
            "\n",
            "Original Text was:\n",
            "'The current government has repeatedly failed to uphold the liberties of the Republic. By concentrating power in the hands of one man, Gaius Julius Caesar, we see the Senate diminished and the voice of the people silenced. Laws are passed without due debate, and magistrates serve at the pleasure of a dictator, not the res publica. If Rome is to remain free, we must reject the tyranny cloaked in promises of order. We must restore the sacred balance between Senate and citizens, consul and tribune.'\n"
          ]
        }
      ]
    }
  ]
}